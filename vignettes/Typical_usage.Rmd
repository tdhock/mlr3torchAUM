<!--
%\VignetteEngine{knitr::knitr}
%\VignetteIndexEntry{Typical usage}
-->

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>")
data.table::setDTthreads(1L)
```

Below we create an imbalanced version of spam data set.

```{r}
spam_task <- mlr3::tsk("spam")
spam_task$filter(1504:4293) # for imbalance.  310    2480 
Class_vec <- spam_task$data(spam_task$row_ids, "type")$type
(count_tab <- table(Class_vec))
```

The output above shows the overall class counts in the data set.
Below we compute the frequencies.

```{r}
count_tab/sum(count_tab)
```

The output above shows there is about 10% spam and 90% nonspam labels overall in the data set.

Below we create a `spam_list` object, which we will use to show how the stratified sampling works.

```{r}
library(data.table)
orig_X_dt <- spam_task$data(spam_task$row_ids, spam_task$col_roles$feature)
sc_X_dt <- data.table(type=Class_vec, scale(orig_X_dt))
spam_sc_task <- mlr3::TaskClassif$new("spam_sc", sc_X_dt, target="type")
spam_sc_task$col_roles$stratum <- "type"
spam_list <- list(task=spam_sc_task)
batch_sampler_class <- mlr3torchAUM::batch_sampler_stratified(
  min_samples_per_stratum = 1)
batch_sampler_instance <- batch_sampler_class(spam_list)
(batch_dt <- as.data.table(t(sapply(
  batch_sampler_instance$batch_list,
  function(i)table(Class_vec[i])
)), key=c("spam","nonspam")))
(batch_size <- batch_sampler_instance$batch_size)
```

The output above shows that 

* there are 310 batches
* all batches have 1 spam label, and 8 nonspam labels,
* so the batch size is 9.

## Comparison with usual batching

To do the usual torch batching, we first create a learner, and use it to derive a dataset:

```{r}
torch_available <- requireNamespace("mlr3torch") && torch::torch_is_installed()
if(torch_available){
  L <- mlr3torch::LearnerTorchMLP$new(task_type="classif")
  spam_dataset <- L$dataset(spam_sc_task)
}
```

Then we compare the two data loading mechanisms:

```{r}
if(torch_available){
  batch_y_dt_list <- list()
  for(set_batch_sampler in c(TRUE, FALSE)){
    spam_loader <- torch::dataloader(
      spam_dataset, batch_size=batch_size, shuffle=TRUE,
      batch_sampler=if(set_batch_sampler)batch_sampler_instance)
    batch.i <- 0
    torch::torch_manual_seed(2)
    coro::loop(for(batch in spam_loader){
      batch.i <- batch.i+1
      batch_y_dt_list[[paste(set_batch_sampler, batch.i)]] <- data.table(
        set_batch_sampler, batch.i,
        type=ifelse(torch::as_array(batch$y$flatten())==1, "spam", "nonspam"))
    })
  }
  (batch_y_dt <- rbindlist(batch_y_dt_list))
  (batch_y_counts <- dcast(batch_y_dt, set_batch_sampler + batch.i ~ type, length))
  batch_y_counts[, table(set_batch_sampler, spam)]
}
```

The output above shows that 

* when `set_batch_sampler=FALSE` (random batching) there are over 100 batches with no positive/spam labels, which could be problematic for learning with complex loss functions that require at least one of each label in each batch (otherwise gradient zero).
* when `set_batch_sampler=TRUE` (stratified batching) each of the 310 batches has one positive/spam label.

## Benchmark comparison

The real test of the sampler is in the context of gradient descent learning, as below.

```{r}
if(torch_available){
  measure_list <- list(
    mlr3torchAUM::MeasureClassifInvAUC$new(),
    mlr3torchAUM::MeasureClassifROCAUM$new())
  batch_sampler_list <- list(
    random_full=mlr3torchAUM::batch_sampler_random(10000),
    stratified_full=mlr3torchAUM::batch_sampler_stratified(10000),
    random_stoch=mlr3torchAUM::batch_sampler_random(9),
    stratified_stoch=mlr3torchAUM::batch_sampler_stratified(1))
  result_for_sampler <- function(batching){
    L <- mlr3torch::LearnerTorchMLP$new(task_type="classif")
    L$loss <- mlr3torchAUM:::nn_ROCAUM_loss
    L$optimizer <- mlr3torch::t_opt("sgd", lr=0.001)
    L$predict_type <- "prob"
    L$callbacks <- mlr3torch::t_clbk("history")
    L$param_set$set_values(
      epochs=20, batch_size=batch_size, seed=1,
      measures_valid=measure_list,
      measures_train=measure_list,
      batch_sampler=batch_sampler_list[[batching]])
    set.seed(1)
    mlr3::set_validate(L, 0.5)
    L$train(spam_sc_task)
    print(L$model$loss_fn)
    list(
      loss=sapply(L$model$loss_fn, torch::as_array),
      history=data.table(batching, L$model$callbacks$history),
      weights=torch::as_array(L$model$network$parameters[["0.weight"]]$flatten()))
  }
  if(FALSE){
    future::plan("multisession")
  }
  LAPPLY <- if(requireNamespace("future.apply")){
    function(x, ...)future.apply::future_lapply(x, ..., future.seed=NULL)
  }else lapply
  result_list <- LAPPLY(names(batch_sampler_list), result_for_sampler)
  names(result_list) <- names(batch_sampler_list)
  print(do.call(data.table, lapply(result_list, "[[", "weights")), nrow=10)
}
```

The result table above shows the weights learned, one column per sampling method.
We see that the first two columns (`*_full`) are identical, which means that the learned weights end up the same after several epochs of gradient descent.
This means we have controlled the randomness:

* the random initialization of the network was the same,
* the random division of train into subtrain/validation was the same,
* and the random selection of batches was the same.

Another result computed above is the number of evaluations of the loss function, which we examine in the code below.

```{r}
do.call(rbind, lapply(result_list, "[[", "loss"))
```

The output above shows that the number of loss function evaluations is 

* the same as the number of epochs for full batching methods.
* over 3000 for stochastic batching methods, but in these cases we have a significant number of batches with loss of zero (no gradient to learn from).

Next, we plot the history of AUM loss and 1-AUC values at each epoch.

```{r}
if(torch_available){
  (history_dt <- rbindlist(lapply(result_list, "[[", "history")))
  (history_long <- melt(
    history_dt,
    measure.vars=measure(set, measure, pattern="(.*).classif.(.*)")
  )[
  , Set := ifelse(set=="train", "subtrain", "validation")
  ][
  , c("sampling","size") := tstrsplit(batching,"_")
  ][])
  if(require(ggplot2)){
    ggplot()+
      theme_bw()+
      geom_hline(aes(
        yintercept=value),
        color="grey",
        data=data.frame(value=0.5, measure="invauc"))+
      theme(text=element_text(size=20))+
      geom_line(aes(
        epoch, value, color=Set),
        linewidth=1,
        data=history_long)+
      facet_grid(measure ~ size + sampling, scales="free", labeller=label_both)+
      scale_y_log10()
  }
}
```

We see in the result figure above that 

* for `size=full` the results are identical, as expected.
* for `size=stoch` the results are similar but stratified sampling is slightly better optimization of subtrain loss (because it sees more batches with non-zero gradients).

Overall, we can see that the batch samplers are working as expected.
